{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Settings and Data Importation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.app.name': 'PIA_final'}, 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>9508</td><td>application_1589299642358_4102</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_4102/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster072.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_4102_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>9519</td><td>application_1589299642358_4113</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_4113/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster065.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_4113_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>9524</td><td>application_1589299642358_4120</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_4120/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster067.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_4120_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>9527</td><td>application_1589299642358_4123</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_4123/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster068.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_4123_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>9528</td><td>application_1589299642358_4124</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_4124/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster066.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_4124_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>9529</td><td>application_1589299642358_4125</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_4125/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster071.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_4125_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>9530</td><td>application_1589299642358_4126</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_4126/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster067.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_4126_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>9532</td><td>application_1589299642358_4128</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_4128/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster068.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_4128_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>9534</td><td>application_1589299642358_4130</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_4130/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster071.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_4130_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>9536</td><td>application_1589299642358_4132</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_4132/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster072.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_4132_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>9537</td><td>application_1589299642358_4133</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_4133/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster071.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_4133_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>9539</td><td>application_1589299642358_4137</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_4137/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster070.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_4137_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>9544</td><td>application_1589299642358_4143</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_4143/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster070.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_4143_01_000002/ebouille\">Link</a></td><td></td></tr><tr><td>9545</td><td>application_1589299642358_4144</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_4144/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster071.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_4144_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>9546</td><td>application_1589299642358_4145</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_4145/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster070.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_4145_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>9547</td><td>application_1589299642358_4146</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_4146/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster068.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_4146_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>9548</td><td>application_1589299642358_4147</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_4147/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster067.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_4147_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>9549</td><td>application_1589299642358_4148</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_4148/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster071.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_4148_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>9550</td><td>application_1589299642358_4149</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_4149/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster070.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_4149_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>9551</td><td>application_1589299642358_4150</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_4150/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster065.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_4150_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>9552</td><td>application_1589299642358_4151</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_4151/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster069.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_4151_01_000001/ebouille\">Link</a></td><td></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure\n",
    "{\"conf\": {\n",
    "    \"spark.app.name\": \"PIA_final\"\n",
    "}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>9553</td><td>application_1589299642358_4152</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_4152/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster070.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_4152_01_000001/ebouille\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "stop_times = spark.read.orc(\"/data/sbb/timetables/orc/stop_times\")\n",
    "routes = spark.read.orc(\"/data/sbb/timetables/orc/routes\")\n",
    "stops = spark.read.orc(\"/data/sbb/timetables/orc/stops\")\n",
    "calendar = spark.read.orc(\"/data/sbb/timetables/orc/calendar\")\n",
    "trips = spark.read.orc(\"/data/sbb/timetables/orc/trips\")\n",
    "transfers = spark.read.orc(\"/data/sbb/timetables/orc/transfers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from geopy import distance\n",
    "import pyspark.sql.functions as functions\n",
    "import networkx as nx\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import from_unixtime, to_date, to_timestamp\n",
    "from pyspark.sql.functions import unix_timestamp\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import TimestampType\n",
    "import datetime\n",
    "import pyspark.sql.functions as F\n",
    "import pandas as pd\n",
    "from math import radians, cos, sin, asin, sqrt\n",
    "from collections import deque\n",
    "from heapq import heappush, heappop\n",
    "import numpy as np\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "from __future__ import print_function\n",
    "from IPython.display import display\n",
    "import itertools\n",
    "#from pyspark.sql.functions import *\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to join each timetable. Each one contains information that we want to filter. \n",
    "Hence for example, here we keep only business days during the week and also we filter non - casual hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "stop_times = stop_times.filter((stop_times.arrival_time <= \"17:00:00\") & (stop_times.departure_time >= \"09:00:00\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "calendar = calendar.filter((calendar.monday == 1) & \n",
    "                           (calendar.tuesday == 1) &\n",
    "                          (calendar.wednesday == 1) &\n",
    "                          (calendar.thursday == 1) &\n",
    "                          (calendar.friday == 1)).drop(*[\"monday\", \"tuesday\", \"wednesday\", \"thursday\", \n",
    "                                                         \"friday\", \"saturday\", \"sunday\", \"start_date\", \"end_date\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we kept only the data that interests us, we can join."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "calendar_trips = calendar.join(trips, on = \"service_id\")\n",
    "calendar_trips = calendar_trips.drop(*['service_id', 'route_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering stops in Zurich"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to retrieve all stations that are inside a 15km radius from Zurich HB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lat_zurich = 47.378177 #Lat and lon for zurich HB\n",
    "long_zurich = 8.540192\n",
    "@functions.udf\n",
    "def is_in_zurich(lat, long):\n",
    "    pos_zurich = (lat_zurich, long_zurich)\n",
    "    pos_to_calculate = (lat, long)\n",
    "    return (distance.distance(pos_zurich, pos_to_calculate).km <= 15.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Filter stops according to our function above and drop unnecessary columns.\n",
    "stops = stops.filter(is_in_zurich(stops.stop_lat, stops.stop_lon) == True)\\\n",
    ".drop(*['location_type', 'parent_station'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "stops_stops = stops.join(stop_times, on=\"stop_id\")\n",
    "joined_data = stops_stops.join(calendar_trips, on=\"trip_id\")\n",
    "joined_data = joined_data.drop(*['pickup_type', 'drop_off_type', 'trip_short_name'])\n",
    "joined_data = joined_data.withColumn('stop_sequence', joined_data.stop_sequence.cast('int'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Public Transport Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here, we aim at having a schedule table. Where one row will contain two stop ID, with a departure time from stop 1 and an arrival time to stop 2.\n",
    "This table will be converted into a graph where an edge is a row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#We use the stop sequence to retrieve the previous station for a trip on the same row as the current one.\n",
    "two_stops_window = Window.partitionBy('trip_id').orderBy('stop_sequence')\n",
    "from_stop = functions.lag('stop_id', 1).over(two_stops_window).alias('from')\n",
    "deptime_stop = functions.lag('departure_time', 1).over(two_stops_window).alias('departure_time_stop')\n",
    "from_name = functions.lag('stop_name', 1).over(two_stops_window).alias('from_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#This function computes the time between two dates in datetime format.\n",
    "@functions.udf\n",
    "def time_between(x, y):\n",
    "    if not x or not y:\n",
    "        return 0\n",
    "    t1 = datetime.datetime.strptime(x, \"%H:%M:%S\")\n",
    "    t2 = datetime.datetime.strptime(y, \"%H:%M:%S\")\n",
    "    return (t1 - t2).seconds/60\n",
    "\n",
    "#Create new columns with hour lag windows.\n",
    "trip_schedule = joined_data.withColumn(\"from\", from_stop)\n",
    "trip_schedule = trip_schedule.withColumn(\"departure_time_stop\", deptime_stop)\n",
    "trip_schedule = trip_schedule.withColumn(\"from_name\", from_name)\n",
    "trip_schedule = trip_schedule.withColumn('distance', time_between(trip_schedule.arrival_time, trip_schedule.departure_time_stop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#remove unnecessary columns, rearrange, rename columns and filter out first of each since each first line have no previous station, no information.\n",
    "trip_schedule = trip_schedule.drop(*[\"trip_headsign\", \"direction_id\", \"departure_time\", \"stop_sequence\", \"stop_lat\", \"stop_lon\"])\n",
    "\n",
    "trip_schedule = trip_schedule.select(\"trip_id\", \"from\", \"from_name\", \"departure_time_stop\",\n",
    "                                     \"stop_id\", \"stop_name\", \"arrival_time\", \"distance\")\\\n",
    ".withColumnRenamed(\"departure_time_stop\", \"departure_time\").withColumnRenamed(\"stop_id\", \"to\").withColumnRenamed(\"stop_name\", \"to_name\")\n",
    "\n",
    "trip_schedule = trip_schedule.where(col(\"from\").isNotNull())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's trip_schedule. Now we need to complete the graph by adding walks and transfers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+-----------+--------------------------+--------------+-------+------------------------------+------------+--------+\n",
      "|trip_id                  |from       |from_name                 |departure_time|to     |to_name                       |arrival_time|distance|\n",
      "+-------------------------+-----------+--------------------------+--------------+-------+------------------------------+------------+--------+\n",
      "|1005.TA.26-131-j19-1.9.H |8503855:0:F|Horgen, Bahnhof           |16:14:00      |8589111|Horgen, Gumelenstrasse        |16:15:00    |1       |\n",
      "|1005.TA.26-131-j19-1.9.H |8589111    |Horgen, Gumelenstrasse    |16:15:00      |8573553|Horgen, Stocker               |16:16:00    |1       |\n",
      "|1005.TA.26-131-j19-1.9.H |8573553    |Horgen, Stocker           |16:16:00      |8573554|Horgen Oberdorf, Bahnhof      |16:18:00    |2       |\n",
      "|1005.TA.26-131-j19-1.9.H |8573554    |Horgen Oberdorf, Bahnhof  |16:18:00      |8573555|Horgen, Bergli                |16:19:00    |1       |\n",
      "|1005.TA.26-131-j19-1.9.H |8573555    |Horgen, Bergli            |16:19:00      |8588985|Horgen, Heubach               |16:20:00    |1       |\n",
      "|1005.TA.26-131-j19-1.9.H |8588985    |Horgen, Heubach           |16:20:00      |8588984|Horgen, Gehren                |16:21:00    |1       |\n",
      "|1087.TA.26-5-B-j19-1.23.R|8591245    |Zürich, Laubegg           |15:42:00      |8591329|Zürich, Saalsporthalle        |15:43:00    |1       |\n",
      "|1087.TA.26-5-B-j19-1.23.R|8591329    |Zürich, Saalsporthalle    |15:43:00      |8591366|Zürich, Sihlcity Nord         |15:45:00    |2       |\n",
      "|1087.TA.26-5-B-j19-1.23.R|8591366    |Zürich, Sihlcity Nord     |15:45:00      |8591415|Zürich, Waffenplatzstrasse    |15:46:00    |1       |\n",
      "|1087.TA.26-5-B-j19-1.23.R|8591415    |Zürich, Waffenplatzstrasse|15:46:00      |8591059|Zürich Enge, Bahnhof/Bederstr.|15:47:00    |1       |\n",
      "+-------------------------+-----------+--------------------------+--------------+-------+------------------------------+------------+--------+\n",
      "only showing top 10 rows"
     ]
    }
   ],
   "source": [
    "trip_schedule.show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding walks between each station"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We focus on the stops table now.\n",
    "We want to compute the distance from each station to each other one in the table and keep only the one that are not 500m away.\n",
    "Since we would need to basically do a double for loop, we need to use spark and cross_join."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "crossed_join = stops.crossJoin(stops.select(stops.stop_id.alias(\"stop_id2\"), stops.stop_name.alias(\"stop_name2\"), \n",
    "                                            stops.stop_lat.alias(\"stop_lat2\"), stops.stop_lon.alias(\"stop_lon2\")))\n",
    "#Remove the diagonal\n",
    "crossed_join = crossed_join.filter(crossed_join.stop_name != crossed_join.stop_name2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Necessary to compute the haversine\n",
    "crossed_join = crossed_join.withColumn(\"stop_lat\", crossed_join.stop_lat.cast(\"float\"))\n",
    "crossed_join = crossed_join.withColumn(\"stop_lon\", crossed_join.stop_lon.cast(\"float\"))\n",
    "crossed_join = crossed_join.withColumn(\"stop_lat2\", crossed_join.stop_lat2.cast(\"float\"))\n",
    "crossed_join = crossed_join.withColumn(\"stop_lon2\", crossed_join.stop_lon2.cast(\"float\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to find out distances between stops, we use the haversine function which is faster than distances from the distance package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@functions.udf\n",
    "def haversine(lon1, lat1, lon2, lat2):\n",
    "    \"\"\"Calculate the great circle distance between two points\n",
    "        on the earth (specified in decimal degrees)\n",
    "    \"\"\"\n",
    "    # convert decimal degrees to radians\n",
    "    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n",
    "    # haversine formula\n",
    "    dlon = lon2 - lon1 \n",
    "    dlat = lat2 - lat1\n",
    "    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
    "    c = 2 * asin(sqrt(a))\n",
    "    r = 6371 # Radius of earth in kilometers\n",
    "    return c * r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can compute the haversine distance and filter the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "crossed_join = crossed_join.withColumn(\"distance_km\", haversine(col(\"stop_lon\"), col(\"stop_lat\"), col(\"stop_lon2\"), col(\"stop_lat2\")))\n",
    "crossed_join = crossed_join.filter(crossed_join.distance_km < 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert into pandas to make our walking_distances dataframe. We can do it because now we have remove a lot of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "stops_df = crossed_join.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make it in the same format as our schedules (our graph). Once this will be computed, we will simply append all the walking edges to the rest of the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "walking_distances = []\n",
    "for _, row in stops_df.iterrows():\n",
    "    #retrieve important informations\n",
    "    current_id, current_name = row[\"stop_id\"], row[\"stop_name\"] \n",
    "    other_id, other_name = row[\"stop_id2\"], row[\"stop_name2\"]\n",
    "    t = float(row[\"distance_km\"]) * 20 #put it into minutes\n",
    "    trip_id = \"w_\"+current_id+\"_\"+other_id\n",
    "    #Keep the same format as the original graph skeletton\n",
    "    walking_distances.append([trip_id, current_id, current_name, None, other_id, other_name, None, t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_walking = pd.DataFrame(data = walking_distances, columns = [\"trip_id\", \"from\", \"from_name\", \"departure_time\", \"to\", \"to_name\", \"arrival_time\", \"distance\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_walking[\"from_name\"] = df_walking[\"from_name\"].apply(lambda x: x.encode('utf-8'))\n",
    "df_walking[\"to_name\"] = df_walking[\"to_name\"].apply(lambda x: x.encode('utf-8'))\n",
    "df_walking = df_walking.astype({'trip_id': 'str', 'from': 'str', 'from_name': 'str',\n",
    "                 'to': 'str', 'to_name': 'str', \n",
    "                 'distance': 'float'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 trip_id     from  ... arrival_time  distance\n",
      "0      w_8500926_8590616  8500926  ...         None  2.447927\n",
      "1      w_8500926_8590737  8500926  ...         None  6.008431\n",
      "2      w_8502186_8502270  8502186  ...         None  9.580292\n",
      "3  w_8502186_8502270:0:1  8502186  ...         None  9.715994\n",
      "4      w_8502186_8590200  8502186  ...         None  9.646272\n",
      "\n",
      "[5 rows x 8 columns]"
     ]
    }
   ],
   "source": [
    "df_walking.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding transfers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transfers are more easy to deal with since we have a table transfers.txt that already contains two stop ID and the time between them. All we need to do is to adapt it to our graph's format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "transfers_df = transfers.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "transfer_times = []\n",
    "for _, row in transfers_df.iterrows():\n",
    "    #Keep important information\n",
    "    from_id, to_id, distance = row[\"from_stop_id\"], row[\"to_stop_id\"], float(row[\"min_transfer_time\"])/60\n",
    "    #distinguish the trip id so that transfers have a unique one\n",
    "    trip_id = \"t_\"+from_id+\"_\"+to_id\n",
    "    transfer_times.append([trip_id, from_id, None, None, to_id, None, None, distance])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      trip_id         from  ... arrival_time distance\n",
      "0   t_8500309:0:2_8500309:0:4  8500309:0:2  ...         None      3.0\n",
      "1   t_8500309:0:2_8500309:0:5  8500309:0:2  ...         None      3.0\n",
      "2   t_8500309:0:2_8500309:0:3  8500309:0:2  ...         None      3.0\n",
      "3   t_8500309:0:2_8500309:0:1  8500309:0:2  ...         None      3.0\n",
      "4  t_8500309:0:2_8500309:0:5B  8500309:0:2  ...         None      3.0\n",
      "\n",
      "[5 rows x 8 columns]"
     ]
    }
   ],
   "source": [
    "df_transfers = pd.DataFrame(data = transfer_times, columns = [\"trip_id\", \"from\", \"from_name\", \"departure_time\", \"to\", \"to_name\", \"arrival_time\", \"distance\"])\n",
    "df_transfers.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have walkings and transfers, let's append them to our final graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    trip_id         from  ... arrival_time distance\n",
      "0  1005.TA.26-131-j19-1.9.H  8503855:0:F  ...     16:15:00        1\n",
      "1  1005.TA.26-131-j19-1.9.H      8589111  ...     16:16:00        1\n",
      "2  1005.TA.26-131-j19-1.9.H      8573553  ...     16:18:00        2\n",
      "3  1005.TA.26-131-j19-1.9.H      8573554  ...     16:19:00        1\n",
      "4  1005.TA.26-131-j19-1.9.H      8573555  ...     16:20:00        1\n",
      "\n",
      "[5 rows x 8 columns]"
     ]
    }
   ],
   "source": [
    "schedule = trip_schedule.toPandas()\n",
    "schedule.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell we simply re arrange each column type so that we indeed have dates, strings etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "schedule[\"from_name\"] = schedule[\"from_name\"].apply(lambda x: x.encode('utf-8'))\n",
    "schedule[\"to_name\"] = schedule[\"to_name\"].apply(lambda x: x.encode('utf-8'))\n",
    "\n",
    "schedule = schedule.astype({'trip_id': 'str', 'from': 'str', 'from_name': 'str',\n",
    "                 'to': 'str', 'to_name': 'str', \n",
    "                 'distance': 'float'})\n",
    "\n",
    "schedule[\"departure_time\"] = schedule[\"departure_time\"].apply(lambda x: datetime.datetime.strptime(x, \"%H:%M:%S\"))\n",
    "schedule[\"arrival_time\"] = schedule[\"arrival_time\"].apply(lambda x: datetime.datetime.strptime(x, \"%H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "temp_graph = schedule.append(df_walking)\n",
    "final_graph = temp_graph.append(df_transfers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "final_graph = final_graph.reset_index().drop(columns = [\"index\"])\n",
    "final_graph[\"departure_time\"] = pd.to_datetime(final_graph.departure_time, format ='%H:%M:%S', errors = 'coerce').dt.time\n",
    "final_graph[\"arrival_time\"] = pd.to_datetime(final_graph.arrival_time, format ='%H:%M:%S', errors = 'coerce').dt.time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    trip_id         from  ... arrival_time distance\n",
      "0  1005.TA.26-131-j19-1.9.H  8503855:0:F  ...     16:15:00      1.0\n",
      "1  1005.TA.26-131-j19-1.9.H      8589111  ...     16:16:00      1.0\n",
      "2  1005.TA.26-131-j19-1.9.H      8573553  ...     16:18:00      2.0\n",
      "3  1005.TA.26-131-j19-1.9.H      8573554  ...     16:19:00      1.0\n",
      "4  1005.TA.26-131-j19-1.9.H      8573555  ...     16:20:00      1.0\n",
      "\n",
      "[5 rows x 8 columns]"
     ]
    }
   ],
   "source": [
    "final_graph.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we convert the graph skeletton to a MultiDiGraph since between two stations (vertex), we have a lot of trip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "g_ = nx.MultiDiGraph()\n",
    "min_time = datetime.time(0, 0, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Final Graph**\n",
    "\n",
    "We model the public transport infrastructure with a graph:\n",
    "* Each stop is a vertex. \n",
    "* Between two stops, we add an edge if any trip goes from one to another. \n",
    "* We consider walkings and transfers. From one vertex, we add an edge to all other vertex that are reachable by walking. \n",
    "* On each edge, we add all the informtion that we have. Departure time, arrival time, weight and trip_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, row in final_graph.iterrows():\n",
    "    g_.add_node(row[\"from\"], max_arrival_time = min_time) #First stop is the first node\n",
    "    g_.add_node(row[\"to\"], max_arrival_time = min_time) #Second node\n",
    "    #Each edges will have all the following information.\n",
    "    #In order to differenciate between all edges between two stations, we add the key as the arrival_time (unique)\n",
    "    g_.add_edge(row[\"from\"], row[\"to\"],\n",
    "               weight = row[\"distance\"],\n",
    "               dep_time = row[\"departure_time\"],\n",
    "               arr_time = row[\"arrival_time\"],\n",
    "               trip_id = row[\"trip_id\"],\n",
    "               key = str(row[\"arrival_time\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Routing Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to find the best route from a point A to a point B we need to first work on the graph.\n",
    "The only information we have is the desired arrival time (and for later the probability of success), the source and the target. We want to output the latest departure time. \n",
    "For the moment our graph is incomplete because some edges (walkings and transfers) do not have any departure / arrival time. \n",
    "\n",
    "* First we do a backward BFS, starting from the target. At each node, we are going to store the maximal_arrival_time at which we should arrive at this node, if we want to be able to catch any trip that will lead us to the target. To do that, we analyze all incoming edges at the visited node and remove edges that have an arrival time bigger than the maximal_arrival_time. After this first filtering, we aim at keeping only 1 edge between two nodes so we keep only the one whose arrival time is the closest to maximal_arrival_time. If we are left with a walking and an trip edge, we keep the one with the smallest weight and update the departure/arrival time of the walking edge, if it is the chosen.\n",
    "\n",
    "* The backward BFS is not enough. It allows us to filter the graph with interesting edges and only 1 edge between two nodes. But we can have edges that have a departure time before the latest arrival time at a node. So we need to do a forward BFS starting from the source. Hence we do another BFS to remove incoherent edges from the source to the target. \n",
    "\n",
    "* To find the shortest path, we apply the djikstra algorithm. Since we are using our graph with networkx, we have to adapt their implementation to ours. Indeed, we needed to keep into consideration the arrival_time and departure time. During the djikstra algorithm we always keep into memory the incoming arrival time and remove edges that could not be taken so for example the ones with a bad departure time. We also penalize waitings at a station by increasing the weight.\n",
    "\n",
    "In the following cells, you will find each algorithm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Takes a datetime.time and returns a datetime.time where we add or substracct the given number of minutes.\n",
    "def substract_times(t1, mins):\n",
    "    t2 = datetime.datetime.combine(datetime.date.today(), t1) - datetime.timedelta(minutes=mins)\n",
    "    return t2.time().replace(microsecond=0)\n",
    "\n",
    "def add_times(t1, mins):\n",
    "    t2 = datetime.datetime.combine(datetime.date.today(), t1) + datetime.timedelta(minutes=mins)\n",
    "    return t2.time().replace(microsecond=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "At a given node, this function filters incoherent edges based on the max_arrival_time attributes. It also compute the departure / arrival time for walking edges.\n",
    "\"\"\"\n",
    "def filter_edges_forward(g, node):\n",
    "    #Retrieve unique outgoing edges\n",
    "    set_edges = set()\n",
    "    for edge in g.out_edges(node):\n",
    "        set_edges.add(edge)\n",
    "    \n",
    "    for e in set_edges:\n",
    "        unique_edge = list(g.get_edge_data(e[0], e[1]).values())[0]\n",
    "        best_time = datetime.time(23, 0, 0)\n",
    "        best_edge = None\n",
    "        #We need to find the lastest arrival_time to the neighbor e[1]\n",
    "        for edge in g.in_edges(e[1]):\n",
    "            ed = g.get_edge_data(edge[0], edge[1]).values()[0]\n",
    "            \n",
    "            if ed.get('arr_time') < best_time:\n",
    "                best_time = ed.get('arr_time')\n",
    "                best_edge = ed\n",
    "        \n",
    "        #Update the new max_arrival_time        \n",
    "        g.nodes[e[1]]['max_arrival_time'] = best_time\n",
    "        \n",
    "        if node == source: #Don't exclude possibilities to arrive earlier\n",
    "            g.nodes[e[0]]['max_arrival_time'] = min_time\n",
    "         \n",
    "        # If we have a walking edge, we assume we can walk as soon as we arrive to the node. \n",
    "        # Hence, departure time is the max_arrival_time at the source walking node.\n",
    "        # Here we also add the weight of 2: a transfer from a station to the walking edge\n",
    "        if unique_edge.get('trip_id')[0] == 'w':\n",
    "            g[e[0]][e[1]]['NaT'][\"dep_time\"] = g.nodes[e[0]]['max_arrival_time']\n",
    "            new_arrival_time = add_times(g.nodes[e[0]]['max_arrival_time'], g[e[0]][e[1]]['NaT'][\"weight\"] + 2)\n",
    "            g[e[0]][e[1]]['NaT'][\"arr_time\"] = new_arrival_time    \n",
    "            #Compute the new weight\n",
    "            g[e[0]][e[1]]['NaT'][\"weight\"] = (datetime.datetime.combine(datetime.date.min, new_arrival_time) \n",
    "                                              - datetime.datetime.combine(datetime.date.min, g[e[0]][e[1]]['NaT'][\"dep_time\"])).seconds / 60\n",
    "        \n",
    "        #Once we updated the max_arrival_time, we can remove trips that departs before the max_arrival_time to the node.\n",
    "        if unique_edge.get('dep_time') < g.nodes[e[0]]['max_arrival_time']:\n",
    "            try:\n",
    "                g.remove_edge(e[0], e[1], str(unique_edge.get('arr_time')))\n",
    "            except:\n",
    "                #Walking have no key.\n",
    "                g.remove_edge(e[0], e[1], 'NaT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "The second BFS algorithm that we apply. This one starts from the source and visit neighbors.\n",
    "\"\"\"\n",
    "def forward_bfs(graph, target):\n",
    "    visited = []\n",
    "    queue = deque()\n",
    "    visited.append(target)\n",
    "    queue.append(target)\n",
    "    while queue:\n",
    "        s = queue.popleft()\n",
    "        filter_edges_forward(graph, s)\n",
    "        for p in graph.neighbors(s):\n",
    "            if p not in visited:\n",
    "                visited.append(p)\n",
    "                queue.append(p)\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "This function updates only walking and transfers edges. It also updates the max_arrival_time of the source node.\n",
    "\"\"\"\n",
    "def update_unique_path(g, e, arrival_time):\n",
    "    g[e[0]][e[1]]['NaT'][\"arr_time\"] = arrival_time\n",
    "    new_departure_time = substract_times(arrival_time, g[e[0]][e[1]]['NaT'][\"weight\"])\n",
    "    g[e[0]][e[1]]['NaT'][\"dep_time\"] = new_departure_time\n",
    "    \n",
    "    if new_departure_time > g.nodes[e[0]][\"max_arrival_time\"]:\n",
    "        g.nodes[e[0]][\"max_arrival_time\"] =  new_departure_time\n",
    "    \n",
    "\"\"\"\n",
    "Function linked to the backward BFS. \n",
    "Filter incoherent and invalid edges incoming to node for a given arrival_time.\n",
    "\"\"\"\n",
    "def filter_edges(g, node, arrival_time):\n",
    "    #For a given node, retrieve its incoming edges.\n",
    "    set_edges = set()\n",
    "    for edge in g.in_edges(node):\n",
    "        set_edges.add(edge)\n",
    "\n",
    "    for e in set_edges:\n",
    "        #Retrieve all edges between two nodes\n",
    "        current = g.get_edge_data(e[0], e[1])\n",
    "        #Make it a list\n",
    "        list_current = list(current.values())\n",
    "        trip_category = list_current[0].get('trip_id')[0]\n",
    "        \n",
    "        if (len(list_current) == 1 and (trip_category == \"w\" or trip_category == \"t\")):\n",
    "            update_unique_path(g, e, arrival_time)\n",
    "        else:\n",
    "            #First we need to find the closest edge to the max_arrival_time\n",
    "            closest = min_time\n",
    "            best_edge = None\n",
    "            best_time = float('inf')\n",
    "            best_time_edge = None\n",
    "            #Here, if an edge have an arrival_time > than max_time_arrival, then this edge is no longer valid. remove it.\n",
    "            for i in list(current.values()):\n",
    "                if i.get('arr_time') > arrival_time:\n",
    "                    g.remove_edge(e[0], e[1], str(i.get('arr_time')))\n",
    "                else:\n",
    "                    #Keep only the closest\n",
    "                    if i.get(\"arr_time\") > closest:\n",
    "                        closest = i.get(\"arr_time\")\n",
    "                        best_edge = i\n",
    "            \n",
    "            # Once we removed all edges that are above the arrival time and defined the closest time\n",
    "            # we repass between all edges remaining: Remove all edges that are not the closest one.\n",
    "            for i in list(current.values()):\n",
    "                if i != best_edge and i.get('trip_id')[0] != \"w\":\n",
    "                    g.remove_edge(e[0], e[1], str(i.get('arr_time')))\n",
    "            \n",
    "            #Since walkings are not filtered above, we may have at most 2 edges (one walking and one trip edge)\n",
    "            if len(list(current.values())) > 1:\n",
    "                # Select the one with the smallest weight\n",
    "                for i in list(current.values()):\n",
    "                    if i.get('weight') < best_time:\n",
    "                        best_time = i.get(\"weight\")\n",
    "                        best_time_edge = i\n",
    "                # Remove the other\n",
    "                for i in list(current.values()):\n",
    "                    if i != best_time_edge:\n",
    "                        if i.get('trip_id')[0] != \"w\":\n",
    "                            g.remove_edge(e[0], e[1], str(i.get('arr_time')))\n",
    "                        else:\n",
    "                            g.remove_edge(e[0], e[1], 'NaT')\n",
    "                    else:\n",
    "                        #If the best edge is a walking edge, update its arrival and departure time that are missing at the beggining.\n",
    "                        if i.get('trip_id')[0] == \"w\":\n",
    "                            g[e[0]][e[1]]['NaT'][\"arr_time\"] = arrival_time\n",
    "                            new_departure_time = substract_times(arrival_time, i[\"weight\"])\n",
    "                            g[e[0]][e[1]]['NaT'][\"dep_time\"] = new_departure_time\n",
    "                            g.nodes[e[0]][\"max_arrival_time\"] =  new_departure_time\n",
    "            \n",
    "            # Update node information: max_arrival_time                \n",
    "            if (len(list(current.values())) != 0) and (list(current.values())[0].get('dep_time') > g.nodes[e[0]][\"max_arrival_time\"]):\n",
    "                g.nodes[e[0]][\"max_arrival_time\"] = list(current.values())[0].get('dep_time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "The first BFS algorithm that we apply. This one starts from the target and visit predecessors.\n",
    "\"\"\"\n",
    "\n",
    "def backward_bfs(graph, target, time):\n",
    "    #Initialy the max arrival time is the query time\n",
    "    graph.nodes[target][\"max_arrival_time\"] = time\n",
    "    visited = []\n",
    "    queue = deque()\n",
    "    visited.append(target)\n",
    "    queue.append(target)\n",
    "    while queue:\n",
    "        s = queue.popleft()\n",
    "        #propagate the info of max_arrival_time\n",
    "        time = graph.nodes[s][\"max_arrival_time\"]\n",
    "        filter_edges(graph, s, time)\n",
    "        for p in graph.predecessors(s):\n",
    "            if p not in visited:\n",
    "                visited.append(p)\n",
    "                queue.append(p)\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we present algorithm that are useful to apply djikstra algorithm to find the shortest path.\n",
    "\n",
    "Our strategy is to modify weights and if we are using a walking edges, we also need to update its departure and arrival time. \n",
    "Now before looking into neighbors of a node, we look at the incoming trip id and arrival time and we filtered even better the graph\n",
    "to remove edges that are not valid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def filter_with_current_time(graph, node, time, t_id):\n",
    "    for e in graph.out_edges(node):\n",
    "        unique_edge = list(graph.get_edge_data(e[0], e[1]).values())[0]\n",
    "        #Add a transfer time if we change bus\n",
    "        if unique_edge.get('trip_id') != t_id:\n",
    "            if t_id[0] != \"w\" and t_id[0] != \"t\":\n",
    "                if unique_edge.get('trip_id')[0] != \"w\" and unique_edge.get('trip_id')[0] != \"t\":\n",
    "                    graph[e[0]][e[1]][str(unique_edge.get('arr_time'))][\"weight\"] = graph[e[0]][e[1]][str(unique_edge.get('arr_time'))][\"weight\"] + 2\n",
    "        \n",
    "        #Also penalize waiting at a stop\n",
    "        if unique_edge.get('trip_id')[0] != \"w\" and unique_edge.get('trip_id')[0] != \"t\":\n",
    "            w = graph[e[0]][e[1]][str(unique_edge.get('arr_time'))][\"weight\"]\n",
    "            graph[e[0]][e[1]][str(unique_edge.get('arr_time'))][\"weight\"] = (datetime.datetime.combine(datetime.date.min, unique_edge.get('dep_time')) \n",
    "                                              - datetime.datetime.combine(datetime.date.min, time)).seconds / 60 + w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Modified Djikstra algorithm\n",
    "\"\"\"\n",
    "\n",
    "def _dijkstra(G, source, get_weight, target, pred=None, paths=None):\n",
    "    G_succ = G.succ if G.is_directed() else G.adj\n",
    "\n",
    "    push = heappush\n",
    "    pop = heappop\n",
    "    dist = {}  # dictionary of final distances\n",
    "    seen = {source: 0}\n",
    "    fringe = []\n",
    "    # Fringe keep in memory the incoming trip_id and arrival_time so that we can add time for transfers and filter edges that do not correspond to \n",
    "    # the arrival time.\n",
    "    push(fringe, (0, source, None, None))\n",
    "    while fringe:\n",
    "        (d_to_current, currentNode, incoming_t_id, incoming_arr_time) = pop(fringe)\n",
    "        if currentNode in dist:\n",
    "            continue  # already searched this node.\n",
    "        dist[currentNode] = d_to_current\n",
    "        if currentNode == target:\n",
    "            break\n",
    "            \n",
    "        if incoming_arr_time is not None: #if we are not at the source\n",
    "            #Filter out going edges before finding the shortest path between them\n",
    "            filter_with_current_time(G, currentNode, incoming_arr_time, incoming_t_id) \n",
    "            # Don't forget to reupdate the G_succ variable.\n",
    "            G_succ = G.succ if G.is_directed() else G.adj\n",
    "\n",
    "        # Deal with each filtered neighbors\n",
    "        for u, e in G_succ[currentNode].items():\n",
    "            #Retrieve information for current outgoing edge\n",
    "            (weight, t_id, dep_time, arr_time) = get_weight(currentNode, u, e)\n",
    "            vu_dist = dist[currentNode] + weight\n",
    "            \n",
    "            if u in dist:\n",
    "                if vu_dist < dist[u]:\n",
    "                    raise ValueError('Contradictory paths found:',\n",
    "                                     'negative weights?')\n",
    "            \n",
    "            elif u not in seen or vu_dist < seen[u]:\n",
    "                seen[u] = vu_dist\n",
    "                \n",
    "                # If we have a transfer or a walking, update the times\n",
    "                if (t_id[0] == \"w\" or t_id[0] == \"t\") and incoming_arr_time is not None:\n",
    "                    arr_time = add_times(incoming_arr_time, weight)\n",
    "                    G[currentNode][u]['NaT'][\"arr_time\"] = arr_time\n",
    "                    G[currentNode][u]['NaT'][\"dep_time\"] = incoming_arr_time\n",
    "                \n",
    "                push(fringe, (vu_dist, u, t_id, arr_time))\n",
    "                paths[u] = paths[currentNode] + [u]\n",
    "        \n",
    "    return (dist, paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Finds the shortest path using dijkstra algorithm from a source to a target.\n",
    "\"\"\"\n",
    "def single_source_dijkstra(G, source, target, weight='weight'):\n",
    "    if source == target:\n",
    "        return ({source: 0}, {source: [source]})\n",
    "\n",
    "    #The function used to retrieve informations from an edge.\n",
    "    if G.is_multigraph():\n",
    "        get_weight = lambda u, v, data: min(\n",
    "            (eattr.get('weight', 1), eattr.get('trip_id', None), eattr.get('dep_time', None), eattr.get('arr_time', None))  for eattr in data.values())\n",
    "    else:\n",
    "        get_weight = lambda u, v, data: data.get('trip_id', 1)\n",
    "\n",
    "    paths = {source: [source]}  # dictionary of paths\n",
    "    return _dijkstra(G, source, get_weight, target=target, paths=paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use all our algorithms to work on the graph and apply the shortest path algorithm on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Finds the shortest path between source and target in graph.\n",
    "\"\"\"\n",
    "def find_my_route(source, target, time, graph):\n",
    "    # Apply first the backward bfs\n",
    "    # Then the forward bfs\n",
    "    # On the final graph, apply dijkstra\n",
    "    temp_graph = backward_bfs(graph, target, time)\n",
    "    final_graph = forward_bfs(temp_graph, source)\n",
    "    shortest_path = single_source_dijkstra(final_graph, source=source, target=target)\n",
    "    return shortest_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictive Model using Delays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import hour, minute, when, array_contains, collect_list\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please uncomment the following cell if you want to create the parquet for the sbb filtered table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#We filtered the sbb dataset to keep only the data with stations around zurich and trains between 9am and 5pm\n",
    "\n",
    "#sbb = spark.read.orc('/data/sbb/orc/istdaten')\n",
    "\n",
    "#We create a list containing all teh stops around zurich using previously generated DF \"stops\"\n",
    "\n",
    "#all_stops = stops.select(\"stop_id\").rdd.map(lambda x : x.stop_id)\n",
    "#all_stops = all_stops.map(lambda x :x).collect()\n",
    "#arr = [stop[:7] for stop in all_stops]\n",
    "#arr = list(np.unique(arr).astype('string'))\n",
    "\n",
    "#We selct only the relevant columns.\n",
    "\n",
    "#sbb = sbb.select('betriebstag','fahrt_bezeichner','haltestellen_name','bpuic' , 'ankunftszeit', 'an_prognose', 'abfahrtszeit','ab_prognose', 'an_prognose_status', 'ab_prognose_status')\n",
    "\n",
    "#We translate the column names to facilitate its use\n",
    "\n",
    "#trad = [(\"betriebstag\", \"date\"), ('fahrt_bezeichner', 'trip'), ('haltestellen_name', 'stop_name'), ('ankunftszeit', 'sched_arrival'), ('an_prognose', 'real_arrival'), ('abfahrtszeit','sched_departure'), ('ab_prognose', 'real_departure'), ('bpuic', 'stop_id')]\n",
    "#for al, en in trad:\n",
    "#    sbb = sbb.withColumnRenamed(al,en)\n",
    "    \n",
    "    \n",
    "#sbb = sbb.filter(col('stop_id').isin(arr))\n",
    "#sbb = sbb.filter((sbb.an_prognose_status == \"PROGNOSE\") | (sbb.an_prognose_status == \"GESCHEATZT\"))\n",
    "#sbb = sbb.filter((sbb.ab_prognose_status == \"PROGNOSE\") | (sbb.ab_prognose_status == \"GESCHEATZT\"))\n",
    "\n",
    "#sbb = sbb.withColumn('sched_arr',to_timestamp(sbb.sched_arrival, 'dd.MM.yyyy HH:mm'))\n",
    "#sbb = sbb.withColumn('real_arr',to_timestamp(sbb.real_arrival, 'dd.MM.yyyy HH:mm'))\n",
    "#sbb = sbb.withColumn('sched_dep',to_timestamp(sbb.sched_departure, 'dd.MM.yyyy HH:mm'))\n",
    "#sbb = sbb.withColumn('real_dep',to_timestamp(sbb.real_departure, 'dd.MM.yyyy HH:mm'))\n",
    "#sbb = sbb.drop(*['sched_arrival', 'real_arrival', 'sched_departure', 'real_departure'])\n",
    "\n",
    "#We keep only rows with non-empty scheduled departure hour or non-empty scheduled arrival hours and filter train out of the work hours (9am-5pm)\n",
    "\n",
    "#sbb = sbb.filter(((col('sched_dep').isNotNull()) & (hour(col('sched_dep')) >= 9) & (hour(col('sched_dep')) <= 17)) | ((col('sched_arr').isNotNull()) & (hour(col('sched_arr')) >= 9) & (hour(col('sched_arr')) <=17)))\n",
    "\n",
    "#We use partition filters to spped up the filtering queries un get_similar and we store it as a parquet file to acces it more quickly \n",
    "\n",
    "#sbb.repartition(\"stop_id\").write.partitionBy(\"stop_id\").parquet('/user/274900/sbb_filtered_partitioned.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#We read the filtered and partitioned file\n",
    "sbb = spark.read.parquet('/user/274900/sbb_filtered_partitioned.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+---------------+------------------+------------------+-------------------+-------------------+-------------------+-------------------+-------+\n",
      "|      date|                trip|      stop_name|an_prognose_status|ab_prognose_status|          sched_arr|           real_arr|          sched_dep|           real_dep|stop_id|\n",
      "+----------+--------------------+---------------+------------------+------------------+-------------------+-------------------+-------------------+-------------------+-------+\n",
      "|18.06.2018|85:3849:190363-02...|Zürich, Central|          PROGNOSE|          PROGNOSE|2018-06-18 09:03:00|2018-06-18 09:05:00|2018-06-18 09:03:00|2018-06-18 09:05:00|8588078|\n",
      "|21.05.2019|85:3849:85919-020...|Zürich, Central|          PROGNOSE|          PROGNOSE|2019-05-21 09:03:00|2019-05-21 09:03:00|2019-05-21 09:03:00|2019-05-21 09:04:00|8588078|\n",
      "|18.06.2018|85:3849:190365-02...|Zürich, Central|          PROGNOSE|          PROGNOSE|2018-06-18 10:18:00|2018-06-18 10:18:00|2018-06-18 10:18:00|2018-06-18 10:19:00|8588078|\n",
      "|21.05.2019|85:3849:85920-020...|Zürich, Central|          PROGNOSE|          PROGNOSE|2019-05-21 09:27:00|2019-05-21 09:28:00|2019-05-21 09:27:00|2019-05-21 09:28:00|8588078|\n",
      "|18.06.2018|85:3849:190367-02...|Zürich, Central|          PROGNOSE|          PROGNOSE|2018-06-18 11:33:00|2018-06-18 11:33:00|2018-06-18 11:33:00|2018-06-18 11:33:00|8588078|\n",
      "|21.05.2019|85:3849:85921-020...|Zürich, Central|          PROGNOSE|          PROGNOSE|2019-05-21 10:18:00|2019-05-21 10:17:00|2019-05-21 10:18:00|2019-05-21 10:17:00|8588078|\n",
      "|18.06.2018|85:3849:190369-02...|Zürich, Central|          PROGNOSE|          PROGNOSE|2018-06-18 12:48:00|2018-06-18 12:46:00|2018-06-18 12:48:00|2018-06-18 12:47:00|8588078|\n",
      "|21.05.2019|85:3849:85922-020...|Zürich, Central|          PROGNOSE|          PROGNOSE|2019-05-21 10:42:00|2019-05-21 10:42:00|2019-05-21 10:42:00|2019-05-21 10:43:00|8588078|\n",
      "|18.06.2018|85:3849:190371-02...|Zürich, Central|          PROGNOSE|          PROGNOSE|2018-06-18 14:03:00|2018-06-18 14:02:00|2018-06-18 14:03:00|2018-06-18 14:02:00|8588078|\n",
      "|21.05.2019|85:3849:85923-020...|Zürich, Central|          PROGNOSE|          PROGNOSE|2019-05-21 11:33:00|2019-05-21 11:32:00|2019-05-21 11:33:00|2019-05-21 11:32:00|8588078|\n",
      "|18.06.2018|85:3849:190373-02...|Zürich, Central|          PROGNOSE|          PROGNOSE|2018-06-18 15:18:00|2018-06-18 15:17:00|2018-06-18 15:18:00|2018-06-18 15:18:00|8588078|\n",
      "|21.05.2019|85:3849:85924-020...|Zürich, Central|          PROGNOSE|          PROGNOSE|2019-05-21 11:57:00|2019-05-21 11:57:00|2019-05-21 11:57:00|2019-05-21 11:57:00|8588078|\n",
      "|18.06.2018|85:3849:190375-02...|Zürich, Central|          PROGNOSE|          PROGNOSE|2018-06-18 16:32:00|2018-06-18 16:32:00|2018-06-18 16:32:00|2018-06-18 16:32:00|8588078|\n",
      "|21.05.2019|85:3849:85925-020...|Zürich, Central|          PROGNOSE|          PROGNOSE|2019-05-21 12:48:00|2019-05-21 12:48:00|2019-05-21 12:48:00|2019-05-21 12:49:00|8588078|\n",
      "|18.06.2018|85:3849:190377-02...|Zürich, Central|          PROGNOSE|          PROGNOSE|2018-06-18 17:47:00|2018-06-18 17:47:00|2018-06-18 17:47:00|2018-06-18 17:48:00|8588078|\n",
      "|21.05.2019|85:3849:85926-020...|Zürich, Central|          PROGNOSE|          PROGNOSE|2019-05-21 13:12:00|2019-05-21 13:11:00|2019-05-21 13:12:00|2019-05-21 13:11:00|8588078|\n",
      "|18.06.2018|85:3849:190398-02...|Zürich, Central|          PROGNOSE|          PROGNOSE|2018-06-18 09:34:00|2018-06-18 09:34:00|2018-06-18 09:35:00|2018-06-18 09:35:00|8588078|\n",
      "|21.05.2019|85:3849:85927-020...|Zürich, Central|          PROGNOSE|          PROGNOSE|2019-05-21 14:03:00|2019-05-21 14:03:00|2019-05-21 14:03:00|2019-05-21 14:04:00|8588078|\n",
      "|18.06.2018|85:3849:190400-02...|Zürich, Central|          PROGNOSE|          PROGNOSE|2018-06-18 10:49:00|2018-06-18 10:49:00|2018-06-18 10:50:00|2018-06-18 10:50:00|8588078|\n",
      "|21.05.2019|85:3849:85928-020...|Zürich, Central|          PROGNOSE|          PROGNOSE|2019-05-21 14:27:00|2019-05-21 14:27:00|2019-05-21 14:27:00|2019-05-21 14:27:00|8588078|\n",
      "+----------+--------------------+---------------+------------------+------------------+-------------------+-------------------+-------------------+-------------------+-------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "sbb.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Given two stops ids and two times, this function returns all the historical \"real\" times of departure from \n",
    "the first station and all the historical \"real\" times of arrival to the second station.\n",
    "\"\"\"\n",
    "def get_similar(stop_id_1,hour_1,minute_1, stop_id_2,hour_2,minute_2, time_travel, list_stops_in_trip = None):\n",
    "    \n",
    "    #Filter with only rows concerning the two stops.\n",
    "    df = sbb.filter((col('stop_id') ==stop_id_1 ) | (col('stop_id') == stop_id_2))\n",
    "    \n",
    "    #keep only rows with non-null values for the departure time for the departure stop and similarly for the arrival stop\n",
    "    df = df.filter(((col('stop_id') ==stop_id_1) & (col('real_dep').isNotNull())) | (col('stop_id') ==stop_id_2) & col('real_arr').isNotNull())\n",
    "    \n",
    "    #add columns to differentiate hours and minutes from the scheduled times\n",
    "    df = df.withColumn('hour_arr',hour(df.sched_arr))\n",
    "    df = df.withColumn('minute_arr', minute(df.sched_arr))\n",
    "    df = df.withColumn('hour_dep',hour(df.sched_dep))\n",
    "    df = df.withColumn('minute_dep', minute(df.sched_dep))\n",
    "    \n",
    "    #keep rows that correspond to departure from the stop1 at hour1 and minute1 (time departure) and similarly for the arrival times and stop2.\n",
    "    df = df.filter(((df.hour_dep == hour_1) & (df.minute_dep == minute_1) & (df.stop_id == stop_id_1)) | ((df.hour_arr == hour_2) & (df.minute_arr >= minute_2) & (df.minute_arr <= minute_2) & (df.stop_id == stop_id_2)))\n",
    "    df = df.withColumn('date', when(col('stop_id') == stop_id_1, df.sched_dep.cast('date')).otherwise(df.sched_arr.cast(\"date\")))\n",
    "    df = df.drop(*['sched_dep', 'sched_arr'])\n",
    "    \n",
    "    #keep only inetersting data wich is the \"real\" departure and arrival times respectively for departure stop and arrival stop\n",
    "    df = df.withColumn('real', when((col('stop_id') == stop_id_1), col('real_dep')).otherwise(col(\"real_arr\"))) \n",
    "    df = df.drop(*['real_arr', 'real_dep'])\n",
    "    \n",
    "    #We pass it to pandas as the dataset is now considerably reduced \n",
    "    df = df.toPandas()\n",
    "    \n",
    "    # We know want to identify the valid trips_id from the sbb dataset that at a given hour link those two stations directly or undireclty.\n",
    "    df_p = df.groupby(['trip', \"date\"]).agg({\"stop_id\": lambda x: set(x)}).reset_index()\n",
    "    df_count = df_p[\"stop_id\"].apply(lambda x : len(x))\n",
    "\n",
    "    trips_df = pd.DataFrame({'trip': df_p[\"trip\"] , 'stops': df_count.values})\n",
    "\n",
    "    #We filter to keep only trips that link the two stations.\n",
    "    valid_trips = trips_df[\"trip\"][trips_df[\"stops\"] == 2]\n",
    "\n",
    "    #We keep a list of those trips\n",
    "    valid_trips = list(np.unique(valid_trips))\n",
    "\n",
    "    #We filter to keep only the rows that correspond to a \"valid_trip\"\n",
    "    df =df[df['trip'].isin(valid_trips)]\n",
    "\n",
    "    df = df.drop([\"stop_id\"], axis = 1)\n",
    "    \n",
    "    #We groupby trip and by date to merge in a list [departure time0, arrival time0] for all the historical data\n",
    "    all_data = df.groupby(['trip', 'date'])[\"real\"].apply(list).values \n",
    "    \n",
    "    #We collect and flatten the data\n",
    "    all_data = list(itertools.chain.from_iterable(all_data))\n",
    "    \n",
    "    size = int(math.floor(len(all_data)/2)*2)\n",
    "    dep = []\n",
    "    arr =[]\n",
    "    \n",
    "    #And separate it into two distinct lists, one for the departure times, the other for the arrival times.\n",
    "    for i in range(0,size, 2):\n",
    "        res = all_data[i + 1] - all_data[i]\n",
    "        if (((res.days) == 0) & ((res.seconds/60) >= time_travel/2) & ((res.seconds/60) <= 3 * time_travel)):\n",
    "            dep.append(all_data[i])\n",
    "            arr.append(all_data[i+1])\n",
    "    \n",
    "    return dep, arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below takes the two lists generated above and computes a probability. We decompose this computation into three cases :\n",
    "* The first case is where we do not change trip ids e.g a trip that consists only of one and unique trip id. \\\n",
    "For this first case, we need the initial stop, the destination stop and the corresponding hours of departure and arrival. \\\n",
    "With get_similar, we retrieve the historical data corresponding the time of departure and time of arrival.\n",
    "The \"weight\" computed is the scheduled time of travel. query_time - hour_2 (scheduled arrival time) corresponds to the margin available for a delay.\n",
    "For the historical data, we compute for each pair, the effective travel time by zipping the arr and dep previously generated and computing the difference.\n",
    "To determine if there is a delay, we compute the difference of the total available time (weight + query_time - hour_2) and substract the effective time that took a journey.\n",
    "If it is < 0, the user will arrive too late. We count such events and divide by the total number of events considered to get a probability\n",
    "  \n",
    "* The second case is a transfer in the same stop from a platform to another platform.\n",
    "We operate in a similar way for the second case. we get all the historical times of arrivals to the \"transfer station\" and the departure times of the train the will be taken afterwards. The available time to do the transfer is therefore the difference between those two. The time needed to do the transfer is \"transfer\" and can be 0 if there is a change of trip ids but not of plateform.\n",
    "* The third case is a transfer by walking between two different stops.\n",
    "This case is similar to the above one except that now, retrieve the data that corresponds to the arrival time to the station where the user will begin his walking and the data that corresponds to the departure times of the train he is willing to take afterwards. The available time again the difference between those two and the needed time to not miss his transfer is \"transfer\" which correspond to the walking time between the two stations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Computes the probablitiy as explained above\n",
    "\"\"\"\n",
    "def get_proba(stop_id_1, hour_1, stop_id_2, hour_2 , query_time, transfer, stop_id_3=None, hour_3= None, stop_id_4= None ,hour_4=None):\n",
    "    \n",
    "    if(transfer == 0 and stop_id_3 == None):\n",
    "        weight = (hour_2 - hour_1).seconds/60\n",
    "        dep, arr = get_similar(stop_id_1, hour_1.hour, hour_1.minute, stop_id_2, hour_2.hour, hour_2.minute, weight)\n",
    "        diff = []\n",
    "        zipped = zip(arr, dep)\n",
    "        for (el1, el2) in zipped:\n",
    "            diff.append((el1 - el2).seconds/60)\n",
    "        delay = []\n",
    "        for time in diff:\n",
    "            delay.append(weight + (query_time - hour_2).seconds/60 - time)\n",
    "            \n",
    "        total_nb = len(delay)\n",
    "        \n",
    "        if total_nb > 0:\n",
    "            proba = float(len([l for l in delay if l > 0])) / total_nb\n",
    "        else: \n",
    "            proba= 1\n",
    "        \n",
    "        return proba\n",
    "        \n",
    "    else:\n",
    "        weight_1 = (hour_2 - hour_1).seconds/60\n",
    "        if (stop_id_3 == stop_id_4):\n",
    "            weight_2 = (hour_3 - hour_2).seconds/60\n",
    "            \n",
    "            _, arr1 = get_similar(stop_id_1, hour_1.hour, hour_1.minute, stop_id_2, hour_2.hour, hour_2.minute, weight_1)\n",
    "            dep1, _ = get_similar(stop_id_2, hour_2.hour, hour_2.minute, stop_id_3,hour_3.hour, hour_3.minute, weight_2)\n",
    "            \n",
    "        else:\n",
    "            weight_3 = (hour_4 - hour_3).seconds/60\n",
    "            _, arr1 = get_similar(stop_id_1, hour_1.hour, hour_1.minute, stop_id_2,hour_2.hour, hour_2.minute, weight_1)\n",
    "            dep1, _ = get_similar(stop_id_3, hour_3.hour, hour_3.minute, stop_id_4,hour_4.hour, hour_4.minute, weight_3)\n",
    "            \n",
    "        diff = []\n",
    "        zipped = zip(dep1, arr1)\n",
    "        for (el1, el2) in zipped:\n",
    "            diff.append((el1 - el2).seconds/60)\n",
    "        delay = []\n",
    "        for time in diff:\n",
    "            delay.append(time-transfer)\n",
    "        \n",
    "        total_nb = len(delay)\n",
    "        \n",
    "        if total_nb > 0:\n",
    "            proba = float(len([l for l in delay if l >0]))/total_nb\n",
    "        else: \n",
    "            proba = 1\n",
    "        \n",
    "        return proba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Robust Route Planning Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to merge the routing algorithm with the predictive model. To do that, here's our strategy:\n",
    "\n",
    "* Since our routing algorithm returns only the latest departure time we need to run it multiple times. This is a valid choice because we filter a lot the graph to obtain the optimal path, hence running it multiple times is quite fast.\n",
    "* For a given time, we define the minimum query time which is by default query_time - 30 minutes. \n",
    "* We aim at generating all paths that arrive between the minimum query time and the default query time, using the routing algorithm.\n",
    "* Then, for each generated paths, according to the success rate that the user wants, we compute its probability of success. And we return the first one that is above the wanted success rate.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Generate all path from source to target with a query time between query_time and query_time - 30 min = min_query_time in the graph.\n",
    "\"\"\"\n",
    "def generate_multiple_paths(source, target, query_time, min_query_time, graph):\n",
    "    # nodes will contain only the stop ids\n",
    "    # details will contain all the detail for each edge\n",
    "    nodes = []\n",
    "    details = []\n",
    "    while query_time >= min_query_time:\n",
    "        gr = graph.copy()\n",
    "        l = find_my_route(source, target, query_time, gr)\n",
    "        \n",
    "        #append in the two lists\n",
    "        if l[1][target] not in nodes:\n",
    "            nodes.append(l[1][target])\n",
    "            details_temp = []\n",
    "            for i in range(1, len(l[1][target])):\n",
    "                details_temp.append(gr.get_edge_data(l[1][target][i - 1], l[1][target][i]).values()[0])\n",
    "            details.append(details_temp)\n",
    "        \n",
    "        # The new query time will be the last arrival time - 1 so that we don't have always the same path generated.\n",
    "        last_arrival = gr.get_edge_data(l[1][target][-2], l[1][target][-1]).values()[0].get('arr_time')\n",
    "        query_time = substract_times(last_arrival, 1)\n",
    "    \n",
    "    return nodes, details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to get the path probability, we need to differenciate multiple cases:\n",
    "\n",
    "* If we are on the portion of the path where we don't change the trip id until the end of the trip, then we can simply compute the probability of arriving late at the target using that trip id. \n",
    "\n",
    "* While the trip_id is the same, we continue until we have a change. Now we have two cases: either we change with a walk between two stations, or we arrive at a station, wait there and continue our trip (no walking). In the first one, we compute if even with a bit of delay, we can still catch the next trip knowing that we have to walk. In the second one, we simply compute the probability of missing the next trip, knowing that we have some time to wait at the station meaning that a trip is able to be late if we are still able to catch the rest of the trip.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Retrieve the total path probability for each edges in the path, by looking at different portion of the path (all changes between trips).\n",
    "\"\"\"\n",
    "def get_path_probability(nodes, edges, query_time):\n",
    "    previous_trip_id = \"\"\n",
    "    result_proba = 1.0\n",
    "    prev_stop_dep = None\n",
    "    prev_stop_arr = None\n",
    "    for i in range(len(edges)):\n",
    "        current_edge = edges[i]\n",
    "        current_trip_id, current_dep_time = current_edge.get('trip_id'), current_edge.get('dep_time')\n",
    "        current_arr_time, current_weight = current_edge.get('arr_time'), current_edge.get('weight')\n",
    "\n",
    "        final_edge = edges[-1]\n",
    "        final_trip_id = final_edge.get('trip_id')\n",
    "        \n",
    "        #First case: If we do not change our trip id until the end\n",
    "        # \n",
    "        if current_trip_id == final_trip_id:\n",
    "            final_arr_time = final_edge.get('arr_time')\n",
    "            current_dep_time = datetime.datetime.combine(date, current_dep_time)\n",
    "            final_arr_time = datetime.datetime.combine(date, final_arr_time)\n",
    "            query_time1 = datetime.datetime.combine(date, query_time)\n",
    "            \n",
    "            #Since the sbb data do not consider plateform, we need to cut our data.\n",
    "            current_stop_dep = nodes[i][:7]\n",
    "            final_stop = nodes[-1][:7]\n",
    "            proba = get_proba(current_stop_dep, current_dep_time, final_stop, final_arr_time, query_time1, 0)\n",
    "            result_proba = result_proba * proba\n",
    "        \n",
    "        if i == 0: # If we are at the first edge, initialize variables\n",
    "            previous_trip_id = current_trip_id\n",
    "            prev_stop_dep = nodes[i]\n",
    "            prev_stop_arr = nodes[i + 1]\n",
    "        else:\n",
    "            # While we have the same trip id, find the first change.\n",
    "            if current_trip_id == previous_trip_id:\n",
    "                prev_stop_dep = nodes[i]\n",
    "                prev_stop_arr = nodes[i + 1]\n",
    "                continue\n",
    "            # If we change with walking\n",
    "            elif (current_trip_id[0] == \"w\" or current_trip_id[0] == \"t\") and i != len(edges) - 1:\n",
    "                stop_dep = nodes[i] #Walk departure\n",
    "                stop_arr = nodes[i + 1] #walk arrival\n",
    "                #need to look at the edge incoming inside the walk, and the one leaving the station after the walk.\n",
    "                past_edge = edges[i - 1]\n",
    "                past_hour_dep = past_edge.get('dep_time')\n",
    "                past_hour_arr = past_edge.get('arr_time')\n",
    "                \n",
    "                next_edge = edges[i + 1]\n",
    "                next_hour_dep = next_edge.get('dep_time')\n",
    "                next_hour_arr = next_edge.get('arr_time')\n",
    "                next_stop_dep = nodes[i + 1]\n",
    "                next_stop_arr = nodes[i + 2]\n",
    "                \n",
    "                #convert into datetime.datetime for the get_proba function\n",
    "                date = datetime.date(1, 1, 1)\n",
    "                past_hour_dep = datetime.datetime.combine(date, past_hour_dep)\n",
    "                past_hour_arr = datetime.datetime.combine(date, past_hour_arr)\n",
    "                next_hour_dep = datetime.datetime.combine(date, next_hour_dep)\n",
    "                next_hour_arr = datetime.datetime.combine(date, next_hour_arr)\n",
    "                \n",
    "                #Cut station id's name for the sbb data\n",
    "                prev_stop_dep_cut = prev_stop_dep[:7]\n",
    "                prev_stop_arr_cut = prev_stop_arr[:7]\n",
    "                next_stop_dep_cut = next_stop_dep[:7]\n",
    "                next_stop_arr_cut = next_stop_arr[:7]\n",
    "                \n",
    "                proba = get_proba(prev_stop_dep_cut, past_hour_dep, prev_stop_arr_cut, past_hour_arr, None, \n",
    "                              current_weight, next_stop_dep_cut, next_hour_dep, next_stop_arr_cut, next_hour_arr)\n",
    "                result_proba = result_proba * proba\n",
    "                \n",
    "                previous_trip_id = current_trip_id\n",
    "                prev_stop_dep = nodes[i]\n",
    "                prev_stop_arr = nodes[i + 1]\n",
    "            \n",
    "            #If we change at a station\n",
    "            elif current_trip_id[0] != \"w\" and current_trip_id[0] != \"t\" and previous_trip_id[0] != \"w\" and previous_trip_id[0] != \"t\":                \n",
    "                past_edge = edges[i - 1]\n",
    "                past_hour_dep = past_edge.get('dep_time')\n",
    "                past_hour_arr = past_edge.get('arr_time')\n",
    "                \n",
    "                date = datetime.date(1, 1, 1)\n",
    "                past_hour_dep = datetime.datetime.combine(date, past_hour_dep)\n",
    "                past_hour_arr = datetime.datetime.combine(date, past_hour_arr)\n",
    "                \n",
    "                \n",
    "                current_arr_time = datetime.datetime.combine(date, current_arr_time)\n",
    "                \n",
    "                prev_stop_arr_c = prev_stop_arr[:7]\n",
    "                prev_stop_dep_c = prev_stop_dep[:7]\n",
    "                \n",
    "                proba = get_proba(prev_stop_dep_c, past_hour_dep, prev_stop_arr_c, past_hour_arr, None, 0, nodes[i + 1][:7], current_arr_time, nodes[i + 1][:7], None)\n",
    "                result_proba = result_proba * proba\n",
    "                \n",
    "                previous_trip_id = current_trip_id\n",
    "                prev_stop_dep = nodes[i]\n",
    "                prev_stop_arr = nodes[i + 1]\n",
    "                \n",
    "            else:\n",
    "                previous_trip_id = current_trip_id\n",
    "                prev_stop_dep = nodes[i]\n",
    "                prev_stop_arr = nodes[i + 1]\n",
    "    return result_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Returns the optimal path with a given success rate.\n",
    "\"\"\"\n",
    "def get_optimal_path(all_paths, details_path, success_rate, query_time):\n",
    "    for (nodes, edges) in zip(all_paths, details_path):\n",
    "        proba = get_path_probability(nodes, edges, query_time)\n",
    "        if proba > success_rate:\n",
    "            return nodes, edges, proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Our final function. Give all the parameter you want for you perfect trip. Returns the optimal path and the probability of success.\n",
    "\"\"\"\n",
    "def robust_journey_planner(source, target, query_time, success_rate):\n",
    "    min_query_time = substract_times(query_time, 30)\n",
    "    \n",
    "    g = g_.copy()\n",
    "    nodes, details = generate_multiple_paths(source, target, query_time, min_query_time, g)\n",
    "    optimal_nodes, optimal_path, success_probability = get_optimal_path(nodes, details, success_rate, query_time)\n",
    "    return optimal_nodes, optimal_path, success_probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build a little IpyWidgets User friendly interface to try robust journey planner. It takes around 1 minute to find the best path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import datetime\n",
    "\"\"\"\n",
    "Function that takes argument coming from local into spark and computes the robust route. It returns a dataFrame that will be shown in the console.\n",
    "\"\"\"\n",
    "def spark_magic(source, target, hour_time, minute_time, success_rate):\n",
    "    query_time = datetime.time(int(hour_time), int(minute_time))\n",
    "    optimal_nodes, optimal_path, success_probability = robust_journey_planner(source, target, query_time, float(success_rate))\n",
    "    list_edges = []\n",
    "    for i in range(len(optimal_path)):\n",
    "        list_edges.append([optimal_path[i].get('trip_id'), optimal_nodes[i], optimal_path[i].get('dep_time'), optimal_nodes[i + 1], optimal_path[i].get('arr_time')])\n",
    "    shortest_path = pd.DataFrame(list_edges, columns = [\"trip_id\", \"from\", \"departure_time\", \"to\", \"arrival_time\"])\n",
    "    shortest_path = spark.createDataFrame(shortest_path.astype(str)) \n",
    "    return shortest_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell builds the user-friendly interface to test our robust journey planner. When you click on the button, it will propagate informations to spark, compute the optimal path and returns a dataframe in the console showing your trip!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49e04de80a304a5bab878f7c5739d9e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Label(value='Source station')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d41648a28d242b6ac907125ace83b6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', placeholder='Source station ID')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bcc2b374297458781647cb9d8c812c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Label(value='Target station')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "639d3653829a4d8888f1a35e49663409",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', placeholder='Target station ID')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b15ab5627a16483aafd0538c42bb8de1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Label(value='Desired arrival time (Hour between 9 and 17)')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bacb82c08ae446adbf28142c44e2fbda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=12, description='Hour:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5d36af3a5204bb9b82994ad0f1796bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=30, description='Minute:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b096ec0260ff4123b22ca9ec9ed7cff8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Label(value='Choose your confidence level')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "429b2cafa0074eeea44fc1f594f33b62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntSlider(value=0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9ed51ff8fb7472a802fcc0185e07011",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Find your route!', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bb58f87d8a147a7b34e080f6bab68dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Label(value='')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a41efcbf3b1403089f28f46c355394a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Label(value='')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6aab301e71754cdfb84efa062c1209ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Label(value='')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%local\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML\n",
    "import datetime\n",
    "import pandas as pd\n",
    "\n",
    "#Source\n",
    "source_label = widgets.Label(\"Source station\")\n",
    "display(source_label)\n",
    "source_textField = widgets.Text(value='', placeholder = \"Source station ID\")\n",
    "display(source_textField)\n",
    "\n",
    "#Target\n",
    "target_label = widgets.Label(\"Target station\")\n",
    "display(target_label)\n",
    "target_textField = widgets.Text(value='', placeholder = \"Target station ID\")\n",
    "display(target_textField)\n",
    "\n",
    "#Arrival time\n",
    "arrival_label = widgets.Label(\"Desired arrival time (Hour between 9 and 17)\")\n",
    "display(arrival_label)\n",
    "arrival_time_hour = widgets.IntText(value=12,description='Hour:', disabled=False)\n",
    "display(arrival_time_hour)\n",
    "arrival_time_minute = widgets.IntText(value=30,description='Minute:', disabled=False)\n",
    "display(arrival_time_minute)\n",
    "\n",
    "#Success rate\n",
    "success_label = widgets.Label(\"Choose your confidence level\")\n",
    "display(success_label)\n",
    "success_rate_slider = widgets.IntSlider()\n",
    "success_slider = widgets.IntSlider()\n",
    "display(success_slider)\n",
    "\n",
    "#Button\n",
    "btn = widgets.Button(description = \"Find your route!\")\n",
    "display(btn)\n",
    "\n",
    "loading = widgets.Label()\n",
    "display(loading)\n",
    "end = widgets.Label()\n",
    "display(end)\n",
    "\n",
    "info = widgets.Label()\n",
    "display(info)\n",
    "\n",
    "def print_result(p):\n",
    "    source = str(source_textField.value)\n",
    "    target = str(target_textField.value)\n",
    "    hour_time = str(arrival_time_hour.value)\n",
    "    minute_time = str(arrival_time_minute.value)\n",
    "    success_rate = str(success_slider.value / 100)\n",
    "    \n",
    "    loading.value = \"Finding your best route...\"\n",
    "    \n",
    "    get_ipython().push(\"source\")\n",
    "    get_ipython().push(\"target\")\n",
    "    get_ipython().push(\"hour_time\")\n",
    "    get_ipython().push(\"minute_time\")\n",
    "    get_ipython().push(\"success_rate\")\n",
    "    \n",
    "    get_ipython().run_cell_magic('send_to_spark', '-i source -n source', ' ')\n",
    "    get_ipython().run_cell_magic('send_to_spark', '-i target -n target', ' ')\n",
    "    get_ipython().run_cell_magic('send_to_spark', '-i hour_time -n hour_time', ' ')\n",
    "    get_ipython().run_cell_magic('send_to_spark', '-i minute_time -n minute_time', ' ')\n",
    "    get_ipython().run_cell_magic('send_to_spark', '-i success_rate -n success_rate', ' ')\n",
    "    \n",
    "    get_ipython().run_cell_magic(\"spark\", \"-o df\", \"df=spark_magic(source, target, hour_time, minute_time, success_rate)\")\n",
    "    df1 = pd.DataFrame(df)\n",
    "    end.value = \"Here it is! (Console)\"\n",
    "    \n",
    "    display(HTML(df1.to_html()))\n",
    "    \n",
    "    dep_hour = str(df1[\"departure_time\"][0].hour)\n",
    "    dep_min = str(df1[\"departure_time\"][0].minute)\n",
    "\n",
    "    arr_hour = str(df1[\"arrival_time\"].iloc[-1].hour)\n",
    "    arr_min = str(df1[\"arrival_time\"].iloc[-1].minute)\n",
    "    info.value = \"You should leave at \"+dep_hour+\":\"+dep_min+\" to arrive at \"+arr_hour +\":\"+arr_min+\" with \" +success_rate+ \" probability of success.\"\n",
    "    \n",
    "btn.on_click(print_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Map visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cells if you want to see the trip on a map!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to load the data from spark so that we can plot using bokeh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%spark -o stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#We need to recompute it since we had it inside the user interface only.\n",
    "shortest_path = spark_magic(source, target, hour_time, minute_time, success_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%spark -o shortest_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "from pyproj import Transformer\n",
    "transformer = Transformer.from_proj(\"EPSG:4326\", \"EPSG:3857\", always_xy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"1001\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  var JS_MIME_TYPE = 'application/javascript';\n",
       "  var HTML_MIME_TYPE = 'text/html';\n",
       "  var EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  var CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    var script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    var cell = handle.cell;\n",
       "\n",
       "    var id = cell.output_area._bokeh_element_id;\n",
       "    var server_id = cell.output_area._bokeh_server_id;\n",
       "    // Clean up Bokeh references\n",
       "    if (id != null && id in Bokeh.index) {\n",
       "      Bokeh.index[id].model.document.clear();\n",
       "      delete Bokeh.index[id];\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            var id = msg.content.text.trim();\n",
       "            if (id in Bokeh.index) {\n",
       "              Bokeh.index[id].model.document.clear();\n",
       "              delete Bokeh.index[id];\n",
       "            }\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    var output_area = handle.output_area;\n",
       "    var output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      var bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      var script_attrs = bk_div.children[0].attributes;\n",
       "      for (var i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      var toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    var events = require('base/js/events');\n",
       "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  \n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    var el = document.getElementById(\"1001\");\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) {\n",
       "        if (callback != null)\n",
       "          callback();\n",
       "      });\n",
       "    } finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.debug(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(css_urls, js_urls, callback) {\n",
       "    if (css_urls == null) css_urls = [];\n",
       "    if (js_urls == null) js_urls = [];\n",
       "\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = css_urls.length + js_urls.length;\n",
       "\n",
       "    function on_load() {\n",
       "      root._bokeh_is_loading--;\n",
       "      if (root._bokeh_is_loading === 0) {\n",
       "        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n",
       "        run_callbacks()\n",
       "      }\n",
       "    }\n",
       "\n",
       "    function on_error() {\n",
       "      console.error(\"failed to load \" + url);\n",
       "    }\n",
       "\n",
       "    for (var i = 0; i < css_urls.length; i++) {\n",
       "      var url = css_urls[i];\n",
       "      const element = document.createElement(\"link\");\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.rel = \"stylesheet\";\n",
       "      element.type = \"text/css\";\n",
       "      element.href = url;\n",
       "      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n",
       "      document.body.appendChild(element);\n",
       "    }\n",
       "\n",
       "    const hashes = {\"https://cdn.bokeh.org/bokeh/release/bokeh-2.0.2.min.js\": \"ufR9RFnRs6lniiaFvtJziE0YeidtAgBRH6ux2oUItHw5WTvE1zuk9uzhUU/FJXDp\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.0.2.min.js\": \"8QM/PGWBT+IssZuRcDcjzwIh1mkOmJSoNMmyYDZbCfXJg3Ap1lEvdVgFuSAwhb/J\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.0.2.min.js\": \"Jm8cH3Rg0P6UeZhVY5cLy1WzKajUT9KImCY+76hEqrcJt59/d8GPvFHjCkYgnSIn\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-2.0.2.min.js\": \"Ozhzj+SI7ywm74aOI/UajcWz+C0NjsPunEVyVIrxzYkB+jA+2tUw8x5xJCbVtK5I\"};\n",
       "\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var element = document.createElement('script');\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.async = false;\n",
       "      element.src = url;\n",
       "      if (url in hashes) {\n",
       "        element.crossOrigin = \"anonymous\";\n",
       "        element.integrity = \"sha384-\" + hashes[url];\n",
       "      }\n",
       "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.head.appendChild(element);\n",
       "    }\n",
       "  };\n",
       "\n",
       "  function inject_raw_css(css) {\n",
       "    const element = document.createElement(\"style\");\n",
       "    element.appendChild(document.createTextNode(css));\n",
       "    document.body.appendChild(element);\n",
       "  }\n",
       "\n",
       "  \n",
       "  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.0.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.0.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.0.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-2.0.2.min.js\"];\n",
       "  var css_urls = [];\n",
       "  \n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    function(Bokeh) {\n",
       "    \n",
       "    \n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if (root.Bokeh !== undefined || force === true) {\n",
       "      \n",
       "    for (var i = 0; i < inline_js.length; i++) {\n",
       "      inline_js[i].call(root, root.Bokeh);\n",
       "    }\n",
       "    if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      var cell = $(document.getElementById(\"1001\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(css_urls, js_urls, function() {\n",
       "      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  \n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  var NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    var el = document.getElementById(\"1001\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error() {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (var i = 0; i < css_urls.length; i++) {\n      var url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    const hashes = {\"https://cdn.bokeh.org/bokeh/release/bokeh-2.0.2.min.js\": \"ufR9RFnRs6lniiaFvtJziE0YeidtAgBRH6ux2oUItHw5WTvE1zuk9uzhUU/FJXDp\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.0.2.min.js\": \"8QM/PGWBT+IssZuRcDcjzwIh1mkOmJSoNMmyYDZbCfXJg3Ap1lEvdVgFuSAwhb/J\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.0.2.min.js\": \"Jm8cH3Rg0P6UeZhVY5cLy1WzKajUT9KImCY+76hEqrcJt59/d8GPvFHjCkYgnSIn\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-2.0.2.min.js\": \"Ozhzj+SI7ywm74aOI/UajcWz+C0NjsPunEVyVIrxzYkB+jA+2tUw8x5xJCbVtK5I\"};\n\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      if (url in hashes) {\n        element.crossOrigin = \"anonymous\";\n        element.integrity = \"sha384-\" + hashes[url];\n      }\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  \n  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.0.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.0.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.0.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-2.0.2.min.js\"];\n  var css_urls = [];\n  \n\n  var inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    function(Bokeh) {\n    \n    \n    }\n  ];\n\n  function run_inline_js() {\n    \n    if (root.Bokeh !== undefined || force === true) {\n      \n    for (var i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\n    if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      var cell = $(document.getElementById(\"1001\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%local\n",
    "from bokeh.io import push_notebook, show, output_notebook, curdoc\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.layouts import layout\n",
    "from bokeh.models import ColumnDataSource, Range1d\n",
    "from bokeh.tile_providers import get_provider, OSM\n",
    "from bokeh.models import Arrow, VeeHead\n",
    "import time\n",
    "\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "  <div class=\"bk-root\" id=\"140a9a56-7ba7-46f9-9cef-5a3f1a33c956\" data-root-id=\"1004\"></div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "(function(root) {\n",
       "  function embed_document(root) {\n",
       "    \n",
       "  var docs_json = {\"7282c063-ef84-4665-a804-39c139aceca2\":{\"roots\":{\"references\":[{\"attributes\":{\"below\":[{\"id\":\"1013\"}],\"center\":[{\"id\":\"1020\"},{\"id\":\"1028\"}],\"left\":[{\"id\":\"1021\"}],\"renderers\":[{\"id\":\"1045\"},{\"id\":\"1050\"},{\"id\":\"1055\"}],\"title\":{\"id\":\"1059\"},\"toolbar\":{\"id\":\"1037\"},\"x_range\":{\"id\":\"1057\"},\"x_scale\":{\"id\":\"1009\"},\"y_range\":{\"id\":\"1058\"},\"y_scale\":{\"id\":\"1011\"}},\"id\":\"1004\",\"subtype\":\"Figure\",\"type\":\"Plot\"},{\"attributes\":{},\"id\":\"1030\",\"type\":\"WheelZoomTool\"},{\"attributes\":{\"bottom_units\":\"screen\",\"fill_alpha\":0.5,\"fill_color\":\"lightgrey\",\"left_units\":\"screen\",\"level\":\"overlay\",\"line_alpha\":1.0,\"line_color\":\"black\",\"line_dash\":[4,4],\"line_width\":2,\"render_mode\":\"css\",\"right_units\":\"screen\",\"top_units\":\"screen\"},\"id\":\"1035\",\"type\":\"BoxAnnotation\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.1},\"fill_color\":{\"value\":\"orange\"},\"line_alpha\":{\"value\":0.1},\"line_color\":{\"value\":\"navy\"},\"size\":{\"units\":\"screen\",\"value\":10},\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"1049\",\"type\":\"Circle\"},{\"attributes\":{\"source\":{\"id\":\"1002\"}},\"id\":\"1056\",\"type\":\"CDSView\"},{\"attributes\":{\"overlay\":{\"id\":\"1035\"}},\"id\":\"1031\",\"type\":\"BoxZoomTool\"},{\"attributes\":{},\"id\":\"1009\",\"type\":\"LinearScale\"},{\"attributes\":{\"data_source\":{\"id\":\"1002\"},\"glyph\":{\"id\":\"1053\"},\"hover_glyph\":null,\"muted_glyph\":null,\"nonselection_glyph\":{\"id\":\"1054\"},\"selection_glyph\":null,\"view\":{\"id\":\"1056\"}},\"id\":\"1055\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"dimension\":\"lon\"},\"id\":\"1014\",\"type\":\"MercatorTicker\"},{\"attributes\":{},\"id\":\"1032\",\"type\":\"SaveTool\"},{\"attributes\":{},\"id\":\"1033\",\"type\":\"ResetTool\"},{\"attributes\":{\"formatter\":{\"id\":\"1016\"},\"ticker\":{\"id\":\"1014\"}},\"id\":\"1013\",\"type\":\"MercatorAxis\"},{\"attributes\":{\"line_color\":{\"value\":\"blue\"},\"line_width\":{\"value\":4},\"x0\":{\"field\":\"x\"},\"x1\":{\"field\":\"x_end\"},\"y0\":{\"field\":\"y\"},\"y1\":{\"field\":\"y_end\"}},\"id\":\"1053\",\"type\":\"Segment\"},{\"attributes\":{\"dimension\":\"lon\"},\"id\":\"1016\",\"type\":\"MercatorTickFormatter\"},{\"attributes\":{},\"id\":\"1011\",\"type\":\"LinearScale\"},{\"attributes\":{\"line_alpha\":{\"value\":0.1},\"line_color\":{\"value\":\"blue\"},\"line_width\":{\"value\":4},\"x0\":{\"field\":\"x\"},\"x1\":{\"field\":\"x_end\"},\"y0\":{\"field\":\"y\"},\"y1\":{\"field\":\"y_end\"}},\"id\":\"1054\",\"type\":\"Segment\"},{\"attributes\":{\"source\":{\"id\":\"1002\"}},\"id\":\"1051\",\"type\":\"CDSView\"},{\"attributes\":{},\"id\":\"1034\",\"type\":\"HelpTool\"},{\"attributes\":{\"text\":\"\"},\"id\":\"1059\",\"type\":\"Title\"},{\"attributes\":{\"active_drag\":\"auto\",\"active_inspect\":\"auto\",\"active_multi\":null,\"active_scroll\":\"auto\",\"active_tap\":\"auto\",\"tools\":[{\"id\":\"1029\"},{\"id\":\"1030\"},{\"id\":\"1031\"},{\"id\":\"1032\"},{\"id\":\"1033\"},{\"id\":\"1034\"},{\"id\":\"1036\"}]},\"id\":\"1037\",\"type\":\"Toolbar\"},{\"attributes\":{\"data_source\":{\"id\":\"1002\"},\"glyph\":{\"id\":\"1048\"},\"hover_glyph\":null,\"muted_glyph\":null,\"nonselection_glyph\":{\"id\":\"1049\"},\"selection_glyph\":null,\"view\":{\"id\":\"1051\"}},\"id\":\"1050\",\"type\":\"GlyphRenderer\"},{\"attributes\":{},\"id\":\"1065\",\"type\":\"Selection\"},{\"attributes\":{\"dimension\":\"lat\"},\"id\":\"1024\",\"type\":\"MercatorTickFormatter\"},{\"attributes\":{\"axis\":{\"id\":\"1013\"},\"ticker\":null},\"id\":\"1020\",\"type\":\"Grid\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.8},\"fill_color\":{\"value\":\"orange\"},\"line_alpha\":{\"value\":0.8},\"line_color\":{\"value\":\"navy\"},\"size\":{\"units\":\"screen\",\"value\":10},\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"1048\",\"type\":\"Circle\"},{\"attributes\":{\"dimension\":\"lat\"},\"id\":\"1022\",\"type\":\"MercatorTicker\"},{\"attributes\":{\"formatter\":{\"id\":\"1024\"},\"ticker\":{\"id\":\"1022\"}},\"id\":\"1021\",\"type\":\"MercatorAxis\"},{\"attributes\":{},\"id\":\"1066\",\"type\":\"UnionRenderers\"},{\"attributes\":{\"tile_source\":{\"id\":\"1003\"}},\"id\":\"1045\",\"type\":\"TileRenderer\"},{\"attributes\":{\"data\":{\"station_name\":[],\"x\":[],\"x_end\":[],\"y\":[],\"y_end\":[]},\"selected\":{\"id\":\"1065\"},\"selection_policy\":{\"id\":\"1066\"}},\"id\":\"1002\",\"type\":\"ColumnDataSource\"},{\"attributes\":{\"axis\":{\"id\":\"1021\"},\"dimension\":1,\"ticker\":null},\"id\":\"1028\",\"type\":\"Grid\"},{\"attributes\":{\"attribution\":\"&copy; <a href=\\\"https://www.openstreetmap.org/copyright\\\">OpenStreetMap</a> contributors\",\"url\":\"https://c.tile.openstreetmap.org/{Z}/{X}/{Y}.png\"},\"id\":\"1003\",\"type\":\"WMTSTileSource\"},{\"attributes\":{},\"id\":\"1029\",\"type\":\"PanTool\"},{\"attributes\":{\"end\":6014714.245624232,\"start\":5993343.190664801},\"id\":\"1058\",\"type\":\"Range1d\"},{\"attributes\":{\"callback\":null,\"tooltips\":[[\"(x,y)\",\"($x, $y)\"],[\"Station name\",\"@station_name\"]]},\"id\":\"1036\",\"type\":\"HoverTool\"},{\"attributes\":{\"end\":957925.5916183513,\"start\":943454.0578152258},\"id\":\"1057\",\"type\":\"Range1d\"}],\"root_ids\":[\"1004\"]},\"title\":\"Bokeh Application\",\"version\":\"2.0.2\"}};\n",
       "  var render_items = [{\"docid\":\"7282c063-ef84-4665-a804-39c139aceca2\",\"notebook_comms_target\":\"1068\",\"root_ids\":[\"1004\"],\"roots\":{\"1004\":\"140a9a56-7ba7-46f9-9cef-5a3f1a33c956\"}}];\n",
       "  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\n",
       "\n",
       "  }\n",
       "  if (root.Bokeh !== undefined) {\n",
       "    embed_document(root);\n",
       "  } else {\n",
       "    var attempts = 0;\n",
       "    var timer = setInterval(function(root) {\n",
       "      if (root.Bokeh !== undefined) {\n",
       "        clearInterval(timer);\n",
       "        embed_document(root);\n",
       "      } else {\n",
       "        attempts++;\n",
       "        if (attempts > 100) {\n",
       "          clearInterval(timer);\n",
       "          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\n",
       "        }\n",
       "      }\n",
       "    }, 10, root)\n",
       "  }\n",
       "})(window);"
      ],
      "application/vnd.bokehjs_exec.v0+json": ""
     },
     "metadata": {
      "application/vnd.bokehjs_exec.v0+json": {
       "id": "1004"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%local\n",
    "lat_zurich = 47.378177\n",
    "long_zurich = 8.540192\n",
    "x = []\n",
    "y = []\n",
    "x_end = []\n",
    "y_end = []\n",
    "station_name = []\n",
    "source_bokeh = ColumnDataSource(data = dict(x=x, y=y, x_end=x_end, y_end=y_end, station_name=station_name))\n",
    "\n",
    "TOOLTIPS = [\n",
    "    (\"(x,y)\", \"($x, $y)\"), \n",
    "    (\"Station name\", \"@station_name\")\n",
    "]\n",
    "\n",
    "# create the map\n",
    "tile_provider = get_provider(OSM)\n",
    "p = figure(x_axis_type=\"mercator\", y_axis_type=\"mercator\", tooltips=TOOLTIPS)\n",
    "p.add_tile(tile_provider)\n",
    "\n",
    "#Add a circle for each station and a segment between pairs of stations\n",
    "p.circle('x', 'y', source=source_bokeh, size=10, line_color=\"navy\", fill_color=\"orange\", alpha=0.8)\n",
    "p.segment(x0='x', y0='y', x1='x_end', y1='y_end',source=source_bokeh,\n",
    "          color=\"blue\", line_width=4)\n",
    "\n",
    "\n",
    "# make the plot centered at Amsterdam\n",
    "x_min, y_min = transformer.transform(long_zurich - 0.065, lat_zurich - 0.065)\n",
    "x_max, y_max = transformer.transform(long_zurich + 0.065, lat_zurich + 0.065)\n",
    "p.x_range = Range1d(x_min, x_max)\n",
    "p.y_range = Range1d(y_min, y_max)\n",
    "\n",
    "t=show(p, notebook_handle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "#shortest_path_1 = shortest_path.toPandas()\n",
    "for i, row in shortest_path.iterrows():\n",
    "    old_x = stops[stops['stop_id'] == str(row['from'])]['stop_lon'].values[0]\n",
    "    old_y = stops[stops['stop_id'] == str(row['from'])]['stop_lat'].values[0]\n",
    "    old_x_end = stops[stops['stop_id'] == str(row['to'])]['stop_lon'].values[0]\n",
    "    old_y_end = stops[stops['stop_id'] == str(row['to'])]['stop_lat'].values[0]\n",
    "    new_x, new_y = transformer.transform(old_x, old_y)\n",
    "    new_x_end, new_y_end = transformer.transform(old_x_end, old_y_end)\n",
    "    x.append(new_x)\n",
    "    y.append(new_y)\n",
    "    x_end.append(new_x_end)\n",
    "    y_end.append(new_y_end)\n",
    "    station_name.append(row[\"from\"])\n",
    "\n",
    "\n",
    "old_last_x = stops[stops['stop_id'] == str(shortest_path.iloc[-1]['to'])]['stop_lon'].values[0]\n",
    "old_last_y = stops[stops['stop_id'] == str(shortest_path.iloc[-1]['to'])]['stop_lat'].values[0]\n",
    "new_last_x, new_last_y = transformer.transform(old_last_x, old_last_y)\n",
    "x.append(new_last_x)\n",
    "y.append(new_last_y)\n",
    "x_end.append(new_last_x)\n",
    "y_end.append(new_last_y)\n",
    "station_name.append(shortest_path.iloc[-1][\"to\"])\n",
    "\n",
    "source_bokeh.data = dict(x=x, y=y,x_end=x_end, y_end=y_end, station_name=station_name)\n",
    "push_notebook(handle=t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided to use the SBB online journey planner to check our results as they probably use the same data as us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SBB online journey planner for trip 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](trip1_im.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sbb planning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"trip1_ls.png\" alt=\"Drawing\" style=\"width: 1000px;\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%local\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML('<img src=\"trip1_ls.png\" alt=\"Drawing\" style=\"width: 1000px;\"/>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our model for trip 1 (Robust)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](trip1_our.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our robust planning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"trip1_us.png\" alt=\"Drawing\" style=\"width: 1000px;\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%local\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML('<img src=\"trip1_us.png\" alt=\"Drawing\" style=\"width: 1000px;\"/>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that for the first trip, our model shows the same stops as the SBB planner. We have the exact same times also"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SBB online journey planner for trip 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](trip2_im.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our model for trip 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](trip2_our.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see a slight difference between our model and the SBB planner. Indeed our model use a different path at the start. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](trip2_ls.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](trip2_ls_our.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But if we look at the departure time we leave at the same time. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
